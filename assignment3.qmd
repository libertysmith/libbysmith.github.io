# Latent Semantic Text Analysis with Quanteda Package in R

### Analyzing Tweets surrounding the Biden-Xi Summit of 2021

Latent Semantic Analysis (LSA) is a technique in natural language processing and text analysis that is used to discover the relationships and associations between words and documents. LSA is primarily applied to large collections of textual data, such as a corpus of documents, to uncover hidden patterns and extract semantic information.

LSA can uncover semantic relationships and associations that go beyond simple keyword matching. It can reduce the "curse of dimensionality" in high-dimensional text data. It is often used for information retrieval, document summarization, and text mining.

LSA has 3 major assumptions. 1) Documents are non-positional ("bag of words"). The "bag of words" approach assumes that the order of the words does not matter. What matters is only the frequency of the single words. 2) Concepts are understood as patterns of words where certain words often go together in similar documents. 3) Words only have one meaning given the contexts surrounding the patterns of words.

To demonstrate LSA, we will implement the technique on text data from 2021 tweets about the Biden and Xi summit.

```{r}
# We will work with the following packages
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(readr)
library(ggplot2)

# The twitter data is coming from a publicly available file on Github
summit <- read_csv("https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv")
```

We can see this Twitter (now X) data has 90 columns containing everything about the Tweets from screenname, reply count, and hashtags. Its a detailed dataset. We do need to take some steps prepping the data to make it readable by the quanteda package for LSA. This includes converting them through the tokenization process. Tokenization involves breaking down a text document into individual units or "tokens," which are typically words, but they can also be subword units like subword pieces in some tokenization models.

After tokenization, the resulting token object is again filtered through a Document Feature Matrix. A Document-Feature Matrix (Also known as Document Term Matrix) that represents the entire corpus of documents. Each row of the DTM corresponds to a document, and each column corresponds to a term (word) in the corpus. The matrix entries represent the frequency of each term in each document or other suitable measures like TF-IDF (Term Frequency-Inverse Document Frequency) scores.

```{r}
# Create new object containing only the text data from tweets
sum_twt = summit$text
toks = tokens(sum_twt)
sumtwtdfm <- dfm(toks)
```

In LSA text analysis involving larger amounts of text or large corpora, we often would include a step called Singular Value Decomposition (SVD). In the SVD step of LSA, the DFM is factorized into three matrices. These matrices are based on the tokenized terms, and the relationships between documents and terms are captured by the vectors associated with the tokens. Resulting in a reduced dimensional object that requires less computational power. This step is included in the command "textmodel_lsa" from the quanteda package executed in the code below.

Below, LSA is applied to the totality of the text data from the Tweets into a document-feature matrix object (sumtwtdfm). The resulting sum_lsa model contains the LSA model of Summit Tweets. The resulting LSA model of the data can then be interpreted based on the a priori needs of the project. We can see from the resulting summary of the LSA object below that the texts contain 145,200 docs (or Tweets in this case), over 160,000 features (AKA dictionary of text), and 10 sk or singular values obtained from the SVD. Singular values play a significant role in Latent Semantic Analysis (LSA) by quantifying the strength of the latent semantic relationships captured during the dimensionality reduction process. The Matrix_low_rank and data statistics components contain the results of the LSA transformation, but they are represented as a large matrix in sparse format. In this case, it's stored as a dgCMatrix object, which is a sparse matrix type in R. The dimensions of the matrix are not provided in the summary, but it's quite large, indicating the transformed LSA space.

```{r}
# Latent Semantic Analysis
sum_lsa <- textmodel_lsa(sumtwtdfm)
summary(sum_lsa)
```

We can specify LSA analysis to look at specific features of the text. Below, this section again tokenizes the tweet data (sum_twt) and creates a document-feature matrix (tweet_dfm) without punctuation. Remember, tokenization breaks the text into individual words or tokens, and dfm() creates a document-feature matrix where rows represent documents (tweets) and columns represent terms. Here, we will focus the functions of the quanteda package on the hashtags in our Tweets. We create a document-feature matrix (tag_dfm) specifically for hashtags by selecting terms that start with "\#". Then we identify the top 50 most frequently occurring hashtags.

```{r}
tweet_dfm <- tokens(sum_twt, remove_punct = TRUE) %>%
  dfm()
head(tweet_dfm)
tag_dfm <- dfm_select(tweet_dfm, pattern = "#*")
toptag <- names(topfeatures(tag_dfm, 50))
head(toptag, 10)
```

Now perhaps we want to do a network analysis of hashtag proliferation surrounding the Summit event. The second line below create a feature co-occurrence matrix (FCM) for hashtags, which captures how often hashtags co-occur in tweets. The next line selects hastags that match the top 50 hashtags from the FCM. Lastly, the final line below generates a network plot of the co-occurrence relationships among the top 50 hashtags. min_freq sets a minimum frequency threshold for displaying hashtags, edge_alpha controls edge transparency, and edge_size adjusts edge thickness.

We can see the bulk of the Twitter discourse containing hastags consisted of topics pertaining to fentanyl and covid-19. There are three other less-frequently discussed hashtag topics that branched off from this discussion. Biden tags branched off into other discussions surrounding Uyghers and Human Rights. Another branch-off emanates from the tag #China which appears to consist of US-Taiwan relations based on the frequency of the tags #US and #Taiwan. The final branch off is the #ccp tag which is the smallest and is paired with the #xijinping. Through this visualization of the hashtag network, we can see how various topics are become related to the initial discussion and can be seen as a family of topics that naturally arise together in public discourse.

```{r}
library("quanteda.textplots")
tag_fcm <- fcm(tag_dfm)
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag)
textplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 5)
```

What if we want to do a network analysis of users in the Twitter discussion? Similar to the hashtag section, we need to create a document-feature matrix (user_dfm) for user mentions by selecting terms that start with "\@". Like before, we identifies the top 50 most frequently mentioned users. Then, create an FCM for user mentions and select user mentions that match the top 50 mentioned users. Finally, we generate a network plot of the co-occurrence relationships among the top 50 mentioned users. min_freq sets a minimum frequency threshold, and other parameters control the appearance of the network plot.

As the plot shows, the bulk of the Tweet discussion is surrounding the users \@capitalonearena, \@nba, \@eneskanter, \@pelicansnba, and \@washwizards

```{r}
user_dfm <- dfm_select(tweet_dfm, pattern = "@*")
topuser <- names(topfeatures(user_dfm, 50))
head(topuser, 20)
user_fcm <- fcm(user_dfm)
head(user_fcm, 20)
user_fcm <- fcm_select(user_fcm, pattern = topuser)
textplot_network(user_fcm, min_freq = 20, edge_color = "firebrick", edge_alpha = 0.8, edge_size = 5)
```

So far we have coded to perform text pre-processing, LSA, and network analysis on Twitter data to explore the relationships between hashtags and user mentions in tweets. The result is a visualization of co-occurrence networks for the most frequent hashtags and user mentions, which can help identify trends and connections in the data. Due to the previously mentioned assumptions for LSA, it has limitations, including difficulty handling polysemy (words with multiple meanings) and the inability to capture context-specific information. More advanced techniques like Word Embeddings (e.g., Word2Vec, GloVe) have become popular alternatives in recent years for capturing semantic relationships in text.

# Keyword-in-Context Analysis and X-ray plots with Quanteda Package in R

### Analyzing Presidential Speeches post 1949

Below, I demonstrate some additional functionalities of the quanteda package where we analyze and visualize textual data, specifically examining the usage of certain keywords in American presidential speeches. We only want to focus on more recent speeches here, as indicated by the second line of code. The KWIC command performs a Keyword-in-Context (KWIC) analysis on the corpus. It looks for instances of the keyword "american" in the speeches and generates an X-ray plot that shows the distribution and context of the keyword in the text. 'textplot_xray()' is a function that visualizes the KWIC results, providing insights into how the keyword is used in the text. The subsequent code uses textplot_xray() to perform KWIC analysis for multiple keywords: "american," "people," and "communist." It compares the usage and context of these keywords in the text. The resulting visualizations show how each keyword is distributed and used in the corpus.

Overall, the code demonstrates how to use the "quanteda" and "quanteda.textstats" packages for text analysis and visualization, focusing on examining the distribution and context of specific keywords in American presidential speeches. The resulting X-ray plots provide a visual representation of keyword usage in the corpus. It appears entering the word "communist" was redundant as never appears in the subset of inaugeral speeches since 1949.

```{r}
# Example extracted from https://quanteda.io/articles/pkgdown/examples/plotting.html
 
library(quanteda.textstats)
data_corpus_inaugural_subset <- 
corpus_subset(data_corpus_inaugural, Year > 1949)
kwic(tokens(data_corpus_inaugural_subset), pattern = "american") %>%
  textplot_xray()


textplot_xray(
  kwic(data_corpus_inaugural_subset, pattern = "american"),
  kwic(data_corpus_inaugural_subset, pattern = "people"),
  kwic(data_corpus_inaugural_subset, pattern = "communist")
)

```

# THE END
